{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-documentintelligence==1.0.0b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "document_intelligence_endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "document_intelligence_key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n",
    "storage_connection_string = os.getenv(\"STORAGE_CONNECTION_STRING\")\n",
    "storage_container_name = os.getenv(\"STORAGE_CONTAINER_NAME\")\n",
    "storage_account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a storage account client to upload blobs (PDFs) to container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Document Intelligence Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=document_intelligence_endpoint, credential=AzureKeyCredential(document_intelligence_key)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Parent Combined PDF page-by-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output_pages\\1.pdf\n",
      "Saved: output_pages\\2.pdf\n",
      "Saved: output_pages\\3.pdf\n",
      "Saved: output_pages\\4.pdf\n",
      "Saved: output_pages\\5.pdf\n",
      "Saved: output_pages\\6.pdf\n",
      "Saved: output_pages\\7.pdf\n",
      "Saved: output_pages\\8.pdf\n",
      "Saved: output_pages\\9.pdf\n",
      "Saved: output_pages\\10.pdf\n",
      "PDF split completed!\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import os\n",
    "\n",
    "# Input PDF file\n",
    "input_pdf_path = r\"C:\\Users\\HP VICTUS\\Downloads\\essencif_AI\\doc_intelli_flow\\finalised_invoice_dataset_disorganised.pdf\"  # Change this to your PDF file\n",
    "output_folder = \"output_pages\"  # Folder to store split pages\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Open the input PDF\n",
    "reader = PdfReader(input_pdf_path)\n",
    "\n",
    "# Loop through each page and save it as a separate file\n",
    "for i, page in enumerate(reader.pages):\n",
    "    writer = PdfWriter()\n",
    "    writer.add_page(page)\n",
    "\n",
    "    output_pdf_path = os.path.join(output_folder, f\"{i+1}.pdf\")\n",
    "    with open(output_pdf_path, \"wb\") as output_pdf:\n",
    "        writer.write(output_pdf)\n",
    "    \n",
    "    print(f\"Saved: {output_pdf_path}\")\n",
    "\n",
    "print(\"PDF split completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading single PDF files to the Azure Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\t1.pdf\n"
     ]
    },
    {
     "ename": "ResourceExistsError",
     "evalue": "The specified blob already exists.\nRequestId:cdf5aae9-f01e-0057-12db-9ed2e1000000\nTime:2025-03-27T05:42:32.9633272Z\nErrorCode:BlobAlreadyExists\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\nRequestId:cdf5aae9-f01e-0057-12db-9ed2e1000000\nTime:2025-03-27T05:42:32.9633272Z</Message></Error>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExistsError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m upload_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file\u001b[38;5;241m=\u001b[39mupload_file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mblob_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\storage\\blob\\_blob_client.py:596\u001b[0m, in \u001b[0;36mBlobClient.upload_blob\u001b[1;34m(self, data, blob_type, length, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m options \u001b[38;5;241m=\u001b[39m _upload_blob_options(\n\u001b[0;32m    581\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    582\u001b[0m     blob_type\u001b[38;5;241m=\u001b[39mblob_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    593\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob_type \u001b[38;5;241m==\u001b[39m BlobType\u001b[38;5;241m.\u001b[39mBlockBlob:\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupload_block_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob_type \u001b[38;5;241m==\u001b[39m BlobType\u001b[38;5;241m.\u001b[39mPageBlob:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m upload_page_blob(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\storage\\blob\\_upload_helpers.py:197\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[1;34m(client, stream, overwrite, encryption_options, blob_settings, headers, validate_content, max_concurrency, length, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         \u001b[43mprocess_storage_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ResourceModifiedError \u001b[38;5;28;01mas\u001b[39;00m mod_error:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite:\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\storage\\blob\\_shared\\response_handlers.py:186\u001b[0m, in \u001b[0;36mprocess_storage_error\u001b[1;34m(storage_error)\u001b[0m\n\u001b[0;32m    183\u001b[0m error\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (error\u001b[38;5;241m.\u001b[39mmessage,)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraise error from None\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# pylint: disable=exec-used # nosec\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\storage\\blob\\_upload_helpers.py:105\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[1;34m(client, stream, overwrite, encryption_options, blob_settings, headers, validate_content, max_concurrency, length, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m     encryption_data, data \u001b[38;5;241m=\u001b[39m encrypt_blob(data, encryption_options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m], encryption_options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    103\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-ms-meta-encryptiondata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m encryption_data\n\u001b[1;32m--> 105\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore [arg-type]\u001b[39;49;00m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjusted_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblob_http_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblob_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_response_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_stream_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjusted_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupload_stream_current\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtier\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblob_tags_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblob_tags_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimmutability_policy_expiry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimmutability_policy_expiry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimmutability_policy_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimmutability_policy_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegal_hold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegal_hold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_hook:\n\u001b[0;32m    122\u001b[0m     progress_hook(adjusted_count, adjusted_count)\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\storage\\blob\\_generated\\operations\\_block_blob_operations.py:858\u001b[0m, in \u001b[0;36mBlockBlobOperations.upload\u001b[1;34m(self, content_length, body, timeout, transactional_content_md5, metadata, tier, request_id_parameter, blob_tags_string, immutability_policy_expiry, immutability_policy_mode, legal_hold, transactional_content_crc64, blob_http_headers, lease_access_conditions, cpk_info, cpk_scope_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[0;32m    855\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m201\u001b[39m]:\n\u001b[1;32m--> 858\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mStorageError, pipeline_response)\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n",
      "File \u001b[1;32mc:\\Users\\HP VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    162\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mResourceExistsError\u001b[0m: The specified blob already exists.\nRequestId:cdf5aae9-f01e-0057-12db-9ed2e1000000\nTime:2025-03-27T05:42:32.9633272Z\nErrorCode:BlobAlreadyExists\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\nRequestId:cdf5aae9-f01e-0057-12db-9ed2e1000000\nTime:2025-03-27T05:42:32.9633272Z</Message></Error>"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(output_folder):\n",
    "    print(file)\n",
    "    blob_client = blob_service_client.get_blob_client(container=storage_container_name, blob=file)\n",
    "    print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + file)\n",
    "\n",
    "    # Upload the created file\n",
    "    upload_file_path = os.path.join(output_folder, file)\n",
    "    with open(file=upload_file_path, mode=\"rb\") as data:\n",
    "        blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a hashMap to keep track of each invoice clubbed by its page number(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashMap={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Function to analyse a PDF file using Azure Doc Intelligence's Analyze API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_intelligence(blob_url, page_number):\n",
    "    \"\"\"Process a single-page PDF and extract invoice details using Azure Document Intelligence.\"\"\"\n",
    "    try:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-invoice\", AnalyzeDocumentRequest(url_source=blob_url)\n",
    "        )\n",
    "        invoices = poller.result()\n",
    "\n",
    "        fields_to_extract = [\"InvoiceId\", \"SubTotal\", \"AmountDue\"]\n",
    "        for invoice in invoices.documents:\n",
    "            for field in fields_to_extract:\n",
    "                field_value = invoice.fields.get(field)\n",
    "                if field_value:\n",
    "                    hashMap[page_number] = field\n",
    "                    print(field)\n",
    "                    if field==\"InvoiceId\" and (\"SubTotal\" in hashMap.values() or \"AmountDue\" in hashMap.values()):\n",
    "                     return \"SubTotal\"\n",
    "                    else:\n",
    "                        return field\n",
    "\n",
    "        # If no relevant fields are found, mark as \"child page\"\n",
    "        hashMap[page_number] = \"child page\"\n",
    "        print(\"child page\")\n",
    "        return \"child page\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_number}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to combine multiple PDF files into a single PDF file - for creating a consolidated invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf_from_pages(input_pdf: str, output_pdf: str, pages: list):\n",
    "    \"\"\"\n",
    "    Creates a new PDF containing only the specified pages from the input PDF.\n",
    "\n",
    "    :param input_pdf: Path to the original multi-page PDF.\n",
    "    :param output_pdf: Path to save the new PDF.\n",
    "    :param pages: List of page numbers (1-based index) to include in the new PDF.\n",
    "    \"\"\"\n",
    "    if not pages:\n",
    "        print(\"No pages specified for creating the PDF.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(input_pdf)\n",
    "        writer = PdfWriter()\n",
    "\n",
    "        for page_num in pages:\n",
    "            if 1 <= page_num <= len(reader.pages):\n",
    "                writer.add_page(reader.pages[page_num - 1])  # Convert 1-based to 0-based index\n",
    "            else:\n",
    "                print(f\"Invalid page number: {page_num}\")\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = \"output_files\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(output_dir, output_pdf), \"wb\") as out_file:\n",
    "            writer.write(out_file)\n",
    "\n",
    "        print(f\"New PDF created: {output_pdf}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PDF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reads a multi-page PDF and sends to Document Intelligence for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str):\n",
    "    \"\"\"Reads a multi-page PDF and sends each page separately to Azure Document Intelligence.\"\"\"\n",
    "    try:\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        hashMap.clear()  # Clear the hashMap before processing\n",
    "        invoiceID = None\n",
    "        pageList = []\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            # Generate the blob URL for the current page\n",
    "            blob_url = f\"https://{storage_account_name}.blob.core.windows.net/{storage_container_name}/{page_num + 1}.pdf\"\n",
    "\n",
    "            # Call Azure Document Intelligence for this page\n",
    "            result = document_intelligence(blob_url, page_num + 1)  # Use 1-based page index\n",
    "            if result in {\"SubTotal\", \"AmountDue\"} and \"InvoiceId\" in hashMap.values():\n",
    "                pageList = list(hashMap.keys())  # Collect all pages in hashMap\n",
    "                if \"InvoiceId\" in hashMap.values():\n",
    "                    # Find the page with \"InvoiceID\" and set it as the invoiceID\n",
    "                    for page, field in hashMap.items():\n",
    "                        print(\"field: {}, page: {}\".format(field, page))\n",
    "                        if field == \"InvoiceId\":\n",
    "                            invoiceID = page\n",
    "                            break\n",
    "\n",
    "                # Reverse the page list and create the output PDF\n",
    "                if invoiceID>pageList[0]:\n",
    "                 pageList.sort(reverse=True)\n",
    "                print(\"pageList:\", pageList)\n",
    "                create_pdf_from_pages(pdf_path, f\"output_{invoiceID}.pdf\", pageList)\n",
    "                print(\"hashMap:\", hashMap)\n",
    "                hashMap.clear()  # Clear the hashMap for the next invoice\n",
    "               \n",
    "\n",
    "        if not pageList:\n",
    "            print(\"No valid invoice data found in the PDF.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the PDF: {e}\")\n",
    "\n",
    "    print(\"\\nProcessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceId\n",
      "SubTotal\n",
      "field: InvoiceId, page: 1\n",
      "pageList: [1, 2]\n",
      "New PDF created: output_1.pdf\n",
      "hashMap: {1: 'InvoiceId', 2: 'SubTotal'}\n",
      "SubTotal\n",
      "InvoiceId\n",
      "field: SubTotal, page: 3\n",
      "field: InvoiceId, page: 4\n",
      "pageList: [4, 3]\n",
      "New PDF created: output_4.pdf\n",
      "hashMap: {3: 'SubTotal', 4: 'InvoiceId'}\n",
      "SubTotal\n",
      "InvoiceId\n",
      "field: SubTotal, page: 5\n",
      "field: InvoiceId, page: 6\n",
      "pageList: [6, 5]\n",
      "New PDF created: output_6.pdf\n",
      "hashMap: {5: 'SubTotal', 6: 'InvoiceId'}\n",
      "InvoiceId\n",
      "SubTotal\n",
      "field: InvoiceId, page: 7\n",
      "pageList: [7, 8]\n",
      "New PDF created: output_7.pdf\n",
      "hashMap: {7: 'InvoiceId', 8: 'SubTotal'}\n",
      "SubTotal\n",
      "InvoiceId\n",
      "field: SubTotal, page: 9\n",
      "field: InvoiceId, page: 10\n",
      "pageList: [10, 9]\n",
      "New PDF created: output_10.pdf\n",
      "hashMap: {9: 'SubTotal', 10: 'InvoiceId'}\n",
      "\n",
      "Processing completed!\n"
     ]
    }
   ],
   "source": [
    "process_pdf(input_pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading consolidated PDFs to Azure Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_1.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\toutput_1.pdf\n",
      "output_10.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\toutput_10.pdf\n",
      "output_4.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\toutput_4.pdf\n",
      "output_6.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\toutput_6.pdf\n",
      "output_7.pdf\n",
      "\n",
      "Uploading to Azure Storage as blob:\n",
      "\toutput_7.pdf\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"output_files\"):\n",
    "    print(file)\n",
    "    blob_client = blob_service_client.get_blob_client(container=storage_container_name, blob=file)\n",
    "    print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + file)\n",
    "\n",
    "    # Upload the created file\n",
    "    upload_file_path = os.path.join(\"output_files\", file)\n",
    "    with open(file=upload_file_path, mode=\"rb\") as data:\n",
    "        blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to call the Prompt FLow endpoint with the consolidated invoice as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def call_prompt_flow_endpoint(url, file_path):\n",
    "    # Ensure the parent folder 'final_output' exists\n",
    "    parent_folder = \"final_output\"\n",
    "    os.makedirs(parent_folder, exist_ok=True)\n",
    "\n",
    "    # Extract the file name without extension from file_path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_stem, file_ext = os.path.splitext(file_name)\n",
    "    \n",
    "    # Create a subfolder named after the file (without extension)\n",
    "    sub_folder = os.path.join(parent_folder, file_stem)\n",
    "    os.makedirs(sub_folder, exist_ok=True)\n",
    "\n",
    "    # Request data\n",
    "    data = {\"url\": url}\n",
    "    body = str.encode(json.dumps(data))\n",
    "\n",
    "    # Load API details from environment variables\n",
    "    url_endpoint = os.getenv(\"PROMPT_FLOW_ENDPOINT\")\n",
    "    api_key = os.getenv(\"PROMPT_FLOW_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': 'Bearer ' + api_key\n",
    "    }\n",
    "\n",
    "    req = urllib.request.Request(url_endpoint, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "        result = response.read()\n",
    "        print(result)\n",
    "\n",
    "        # Decode and parse JSON\n",
    "        response_json = json.loads(result.decode(\"utf-8\"))\n",
    "\n",
    "        # Extract markdown text and JSON structure\n",
    "        markdown_text = response_json[\"output\"][\"markdown_text\"]\n",
    "        json_struct = response_json[\"output\"][\"json_struct\"]\n",
    "\n",
    "        # Save Markdown file\n",
    "        md_file_path = os.path.join(sub_folder, f\"{file_stem}.md\")\n",
    "        with open(md_file_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "            md_file.write(markdown_text)\n",
    "\n",
    "        # Save JSON file\n",
    "        json_file_path = os.path.join(sub_folder, f\"{file_stem}.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(json_struct, json_file, indent=4)\n",
    "\n",
    "        # Copy the original PDF file to the subfolder\n",
    "        pdf_destination_path = os.path.join(sub_folder, file_name)\n",
    "        shutil.copy(file_path, pdf_destination_path)\n",
    "\n",
    "        # Create a ZIP archive of the subfolder\n",
    "        zip_file_path = os.path.join(parent_folder, f\"{file_stem}.zip\")\n",
    "        shutil.make_archive(zip_file_path.replace(\".zip\", \"\"), 'zip', sub_folder)\n",
    "\n",
    "        print(f\"Files saved in: {sub_folder}\")\n",
    "        print(f\"- {md_file_path}\")\n",
    "        print(f\"- {json_file_path}\")\n",
    "        print(f\"- {pdf_destination_path}\")\n",
    "        print(f\"Zipped Folder: {zip_file_path}\")\n",
    "\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "        print(error.info())\n",
    "        print(error.read().decode(\"utf8\", 'ignore'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://essencifaidemostr.blob.core.windows.net/filesnew/output_1.pdf\n",
      "b'{\"output\":{\"json_struct\":[{\"AmountDue\":1030001.0,\"CustomerName\":\"XYZ Enterprises\",\"DueDate\":\"2025-04-10\",\"InvoiceDate\":\"2025-03-26\",\"InvoiceId\":\"INV-2025-00001\",\"InvoiceTotal\":1030001.0,\"Items\":[{\"Amount\":50000.0,\"Description\":\"Cloud Consulting\\\\nServices\",\"Quantity\":10,\"UnitPrice\":5000.0},{\"Amount\":40000.0,\"Description\":\"Azure AI Integration\",\"Quantity\":5,\"UnitPrice\":8000.0},{\"Amount\":7000.0,\"Description\":\"Microsoft Fabric\\\\nImplementation\",\"Quantity\":1,\"UnitPrice\":7000.0},{\"Amount\":6000.0,\"Description\":\"Ongoing Support\",\"Quantity\":1,\"UnitPrice\":6000.0}],\"VendorAddress\":{\"_data\":{\"city\":\"Mumbai\",\"countryRegion\":\"India\",\"houseNumber\":\"123\",\"road\":\"Business Street\",\"streetAddress\":\"123 Business Street\"}},\"VendorName\":\"ABC Consulting Ltd.\"}],\"markdown_text\":\"# Invoice Details\\\\n\\\\n## Vendor Information\\\\n- **Vendor Name:** ABC Consulting Ltd.\\\\n- **Vendor Address:**  \\\\n  123 Business Street  \\\\n  Mumbai, India\\\\n\\\\n## Customer Information\\\\n- **Customer Name:** XYZ Enterprises\\\\n\\\\n## Invoice Information\\\\n- **Invoice ID:** INV-2025-00001  \\\\n- **Invoice Date:** March 26, 2025  \\\\n- **Due Date:** April 10, 2025  \\\\n- **Invoice Total:** \\\\u20b91,030,001.00  \\\\n- **Amount Due:** \\\\u20b91,030,001.00  \\\\n\\\\n## Items\\\\n| Description                    | Quantity | Unit Price (\\\\u20b9) | Amount (\\\\u20b9)  |\\\\n|--------------------------------|----------|----------------|-------------|\\\\n| Cloud Consulting Services      | 10       | 5,000.00       | 50,000.00   |\\\\n| Azure AI Integration           | 5        | 8,000.00       | 40,000.00   |\\\\n| Microsoft Fabric Implementation | 1       | 7,000.00       | 7,000.00    |\\\\n| Ongoing Support                | 1        | 6,000.00       | 6,000.00    |\\\\n\\\\n## Notes\\\\nPlease ensure payment is made by the due date to avoid late fees.\"}}\\n'\n",
      "Files saved in: final_output\\output_1\n",
      "- final_output\\output_1\\output_1.md\n",
      "- final_output\\output_1\\output_1.json\n",
      "- final_output\\output_1\\output_1.pdf\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"output_files\"):\n",
    "    blob_url = f\"https://{storage_account_name}.blob.core.windows.net/{storage_container_name}/{file}\"\n",
    "    print(blob_url)\n",
    "    call_prompt_flow_endpoint(blob_url, file_path=\"output_files/\"+file)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
